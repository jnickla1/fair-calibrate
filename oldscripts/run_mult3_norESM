#!/bin/bash
#SBATCH -J FAIRc_NORESM
#SBATCH -t 0:20:00
#SBATCH -n 5
#SBATCH -N 1
####2-7
#SBATCH --account=epscor-condo
#SBATCH --mem-per-cpu=25gb

# Use '%A' for array-job ID, '%J' for job ID and '%a' for task ID
#####SBATCH -e arrayjob-%a.err
#####SBATCH -o arrayjob-%a.out

#echo "Starting job $SLURM_ARRAY_TASK_ID on $HOSTNAME"

# usage: ./run

eval "$(conda shell.bash hook)"
conda activate fair-calibrate
module load r
module load parallel
# Don't forget to set your environment variables in .env
if [ -f .env ]
then
  export $(cat .env | xargs)
fi

# Base path to your fair-calibrate input tree
script_dir=$PWD
export base_dir="${script_dir}/input/fair-${FAIR_VERSION}/v${CALIBRATION_VERSION}/${CONSTRAINT_SET}"

unset ENSEMBLE_RUN
export ENSEMBLE_RUN=1


unset CUTOFF_YEAR
export start=$SECONDS

#cd "${base_dir}/sampling"
#"./09_run-fair-ssp-prior-ensemble-ebm3-intvar.py" || { echo "ERROR in final sampling for cutoff"; exit 1; }
#echo $(( SECONDS - start ))
#takes about 8600s for 40
#echo "Done with sampling"

# Define a function to run per cutoff year
run_cutoff() {
  export CUTOFF_YEAR=$1
  echo ">>> Constraining with cutoff year = ${CUTOFF_YEAR}"
  cd "${base_dir}/constraining" || exit 1

  file_array=( $(\ls [0-9][0-9]_*.*) )
  for filekey in "${!file_array[@]}"; do
    echo $filekey
    if [ $filekey -lt 4 ]; then  # debugging constraint files
      file=${file_array[$filekey]}
      #echo "####  â€¢ constraining/${file}####"
      #echo $((SECONDS - start))
      chmod +x "$file"
      "./$file" || { echo "ERROR in constraining/${file} for cutoff ${CUTOFF_YEAR}"; exit 1; }
    fi
  done
  echo $CUTOFF_YEAR
  echo $((SECONDS - start))
}

export -f run_cutoff  # export the function to subshells

# Run in parallel across years
parallel -j 4 run_cutoff ::: $(seq 1930 1933)
#2099


echo $(( SECONDS - start ))
echo "All done!"
